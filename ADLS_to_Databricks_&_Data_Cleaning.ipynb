{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b4b134-50e6-47f9-8d9f-d52cf0f4687c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "###  Configure the connection between Azure Data Lake Storage  (ADLS) and Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b9685a-80cc-4004-bc37-286119f0b971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"olistdatastorageaccountd\"\n",
    "application_id = \"615d8082-bfcb-4fd8-8f69-e1089ea2a457\"\n",
    "directory_id = \"39bafab8-b8b9-4cfc-8316-5116be5db767\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", application_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", \"SDg8Q~986FB3IdCu4bDdTnfifTFQjr7TCttbpbvJ\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2cdcd55-4a58-4e39-80ce-de841326c6be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Validate the connection by reading a file from ADLS Bronze folder using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460faffd-29ed-43f1-94f5-1b28079326b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## df = spark.read.csv(f\"abfss://container@datalakestorage.dfs.core.windows.net/Bronze/ olist_customers_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "customer_df = spark.read.csv(f\"abfss://olistdata@olistdatastorageaccountd.dfs.core.windows.net/Bronze/olist_customers_dataset.csv\", header=True, inferSchema=True)\n",
    "display(customer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0febe5-cb70-4c54-a057-e2c8fb4dfd9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Read Bronze layer Raw data into Spark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf1a3cac-f70d-458c-896b-7ae4962c964f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"abfss://olistdata@olistdatastorageaccountd.dfs.core.windows.net/Bronze/\"\n",
    "\n",
    "orders_path = base_path + \"olist_orders_dataset.csv\"\n",
    "payments_path = base_path + \"olist_order_payments.csv\"\n",
    "reviews_path = base_path + \"olist_order_reviews_dataset.csv\"\n",
    "items_path = base_path + \"olist_order_items_dataset.csv\"\n",
    "customers_path = base_path + \"olist_customers_dataset.csv\"\n",
    "sellers_path = base_path + \"olist_sellers_dataset.csv\"\n",
    "geolocation_path = base_path + \"olist_geolocation_dataset.csv\"\n",
    "products_path = base_path + \"olist_products_dataset.csv\"\n",
    "\n",
    "orders_df = spark.read.csv(orders_path, header=True, inferSchema=True)\n",
    "payments_df = spark.read.csv(payments_path, header=True, inferSchema=True)  \n",
    "reviews_df = spark.read.csv(reviews_path, header=True, inferSchema=True)\n",
    "items_df = spark.read.csv(items_path, header=True, inferSchema=True)\n",
    "customers_df = spark.read.csv(customers_path, header=True, inferSchema=True)\n",
    "sellers_df = spark.read.csv(sellers_path, header=True, inferSchema=True)\n",
    "geolocation_df = spark.read.csv(geolocation_path, header=True, inferSchema=True)\n",
    "products_df = spark.read.csv(products_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d9050e-063f-4c12-8744-e6dd7b869cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Validate the mongo DB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba6e9968-9ce8-4ad0-aa83-fc7a42b8365f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing module\n",
    "from pymongo import MongoClient\n",
    "\n",
    "hostname = \"q9g8j.h.filess.io\"\n",
    "database = \"olistDataNoSQL_thencheese\"\n",
    "port = \"61004\"\n",
    "username = \"olistDataNoSQL_thencheese\"\n",
    "password = \"916bc2473e798b3db3f94215c2b576b27b57fb3b\"\n",
    "\n",
    "uri = \"mongodb://\" + username + \":\" + password + \"@\" + hostname + \":\" + port + \"/\" + database\n",
    "\n",
    "# Connect with the portnumber and host\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# Access database\n",
    "mydatabase = client[database]\n",
    "display(mydatabase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e7445e-6df5-4ac1-b446-1d3f5c60bc72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Read Mongo DB data into spark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daeaf6d3-93f3-454e-af3b-5ac402d45bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "collection = mydatabase['product_categories']\n",
    "mongo_data = pd.DataFrame(list(collection.find()))\n",
    "\n",
    "mongo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40f1714-f4a0-443c-bbf5-d8818500b2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop the _id column before converting to Spark dataset\n",
    " \n",
    "mongo_data.drop(\"_id\",axis=1,inplace=True)\n",
    "mongo_spark_df = spark.createDataFrame(mongo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b7d9f17-52da-4bb8-a8b0-f5936901a54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cleaning Data\n",
    "#### Basic cleaning like \n",
    "####### 1. Removing Dups\n",
    "####### 2. Converting data types\n",
    "####### 3. Adding new aggregated column derived from other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7081439-452e-4cb8-9b9d-726dfec1695e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import basic functions for the cleaning and transformation\n",
    "from pyspark.sql.functions import col, to_date, datediff, current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77d6805-9c79-4362-9384-41794edb1aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to remove the duplicates in the datasets\n",
    "def clean_dataFrame(df,name):\n",
    "    print(\"Cleaning:\"+name)\n",
    "    return df.dropDuplicates().na.drop()\n",
    "\n",
    "orders_df=clean_dataFrame(orders_df,\"orders\")\n",
    "display(orders_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91344551-1153-4b47-8304-51babc79b2d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert Date Columns from timestamp datatype to date datatype. \n",
    "\n",
    "orders_df = orders_df.withColumn(\"order_purchase_timestamp\", to_date(\"order_purchase_timestamp\"))\\\n",
    "                     .withColumn(\"order_delivered_customer_date\", to_date(\"order_delivered_customer_date\"))\\\n",
    "                     .withColumn(\"order_estimated_delivery_date\", to_date(\"order_estimated_delivery_date\"))\n",
    "\n",
    "display(orders_df)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce146f8-5325-4531-b6d0-6e42e60cd8b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate delivery & Time delays\n",
    "\n",
    "orders_df = orders_df.withColumn(\"actual_delivery_time\", datediff((\"order_delivered_customer_date\"), (\"order_purchase_timestamp\")))\n",
    "\n",
    "orders_df = orders_df.withColumn(\"estimated_delivery_time\", datediff((\"order_estimated_delivery_date\"), (\"order_purchase_timestamp\")))\n",
    "\n",
    "orders_df = orders_df.withColumn(\"delay Time\", (col(\"actual_delivery_time\") - col(\"estimated_delivery_time\")))\n",
    "\n",
    "display(orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdef4895-4570-44cd-b671-670859d1dcd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Joining multiple dataframes to form a single dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce4e8db9-89c2-487b-a3b5-c37214554beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Joining the Raw datasets from Bronze folder post Data Cleansing\n",
    "\n",
    "orders_customers_df = orders_df.join(customers_df, orders_df.customer_id == customers_df.customer_id, \"left\")\n",
    "\n",
    "orders_payments_df = orders_customers_df.join(payments_df, orders_customers_df.order_id == payments_df.order_id, \"left\")\n",
    "\n",
    "orders_items_df = orders_payments_df.join(items_df,\"order_id\", \"left\")\n",
    "\n",
    "orders_items_products_df = orders_items_df.join(products_df, orders_items_df.product_id == products_df.product_id, \"left\")\n",
    "\n",
    "final_df = orders_items_products_df.join(sellers_df, orders_items_products_df.seller_id == sellers_df.seller_id, \"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ea262f-4d0c-4082-9611-1e7101be9f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68ee307-adde-4a1e-91b5-2a04630ced27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5f8988-2927-4cb3-8ed2-99775a2fa585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Joining the final dataframe with the Mongodb dataframe\n",
    "\n",
    "final_df = final_df.join(mongo_spark_df,\"product_category_name\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1f6854-d36e-4e88-8514-94bb2c2da9e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1f0522a-a0c5-4194-bf1c-8d32db9a46eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ADLS_to_Databricks_&_Data_Cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}